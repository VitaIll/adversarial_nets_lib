{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"An issue occurred while importing 'torch-sparse'\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"An issue occurred while importing 'torch-cluster'\")\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.utils import dropout_edge\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from adversarial_nets import AdversarialEstimator, GraphDataset\n",
    "\n",
    "\n",
    "def build_peer_operator(A: np.ndarray, row_normalize: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Make a peer operator P from adjacency A with zero diagonal.\n",
    "    Optionally row-normalize by degree; isolates pure peer effects and avoids self-loops.\n",
    "    \"\"\"\n",
    "    P = A.copy().astype(float)\n",
    "    np.fill_diagonal(P, 0.0)\n",
    "    if row_normalize:\n",
    "        deg = P.sum(axis=1, keepdims=True)\n",
    "        # Safe normalization: rows with zero degree remain zeros\n",
    "        with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "            P = np.divide(P, deg, out=np.zeros_like(P), where=(deg > 0))\n",
    "    return P\n",
    "\n",
    "def simulate_linear_in_means(X: np.ndarray,\n",
    "                             A: np.ndarray,\n",
    "                             theta: np.ndarray,\n",
    "                             noise_std: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ground-truth simulator: y = (I - rho P)^{-1} (alpha*1 + beta*X + gamma*P X + eps)\n",
    "    X: (n, 1) single scalar feature per node (for simplicity)\n",
    "    A: (n, n) adjacency\n",
    "    theta: [alpha, beta, gamma, rho]\n",
    "    \"\"\"\n",
    "    alpha, beta, gamma, rho = map(float, theta)\n",
    "    n = X.shape[0]\n",
    "    P = build_peer_operator(A, row_normalize=True)\n",
    "    I = np.eye(n)\n",
    "\n",
    "    # Right-hand side: alpha*1 + beta*x + gamma*P*x + eps\n",
    "    rhs = alpha * np.ones(n) + beta * X[:, 0] + gamma * (P @ X[:, 0]) + np.random.normal(0.0, noise_std, size=n)\n",
    "\n",
    "    # Solve (I - rho P) y = rhs\n",
    "    y = np.linalg.solve(I - rho * P, rhs)\n",
    "    return y\n",
    "\n",
    "def structural_model(x: np.ndarray, adjacency: np.ndarray, y0: np.ndarray, theta: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Structural mapping required by AdversarialEstimator.\n",
    "    Ignores y0 here (kept for API compatibility).\n",
    "    \"\"\"\n",
    "    # x expected (n, k). We assume k=1 for simplicity; if k>1, use x[:,0] or adapt the formula.\n",
    "    return simulate_linear_in_means(x[:, [0]], adjacency, theta, noise_std=0.0)\n",
    "\n",
    "\n",
    "def discriminator_factory(input_dim, hidden_dim=6, num_classes=2):\n",
    "    \"\"\"\n",
    "    A slightly more expressive yet tiny GNN:\n",
    "    - Two GCN layers (same width) with LayerNorm + ReLU\n",
    "    - Edge dropout for robustness\n",
    "    - Lightweight residual connection\n",
    "    - Compact MLP head\n",
    "    Default hidden_dim=6 keeps params roughly ~O(input_dim*6) and under ~200 for small inputs.\n",
    "    \"\"\"\n",
    "    class RobustTinyGNN(nn.Module):\n",
    "        def __init__(self, in_dim, hid_dim, num_cls):\n",
    "            super().__init__()\n",
    "            self.conv1 = GCNConv(in_dim, hid_dim, add_self_loops=False, normalize=True)\n",
    "            self.ln1   = nn.LayerNorm(hid_dim)\n",
    "            self.conv2 = GCNConv(hid_dim, hid_dim, add_self_loops=False, normalize=True)\n",
    "            self.ln2   = nn.LayerNorm(hid_dim)\n",
    "            self.readout = nn.Sequential(\n",
    "                nn.Linear(hid_dim, 4),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(4, num_cls)\n",
    "            )\n",
    "\n",
    "        def forward(self, data):\n",
    "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "            # Drop a small fraction of edges during training for robustness.\n",
    "            edge_index, _ = dropout_edge(edge_index, p=0.1, training=self.training)\n",
    "\n",
    "            # GCN block 1\n",
    "            h = self.conv1(x, edge_index)\n",
    "            h = self.ln1(h)\n",
    "            h = F.relu(h, inplace=True)\n",
    "            h = F.dropout(h, p=0.2, training=self.training)\n",
    "\n",
    "            # GCN block 2 + lightweight residual\n",
    "            h2 = self.conv2(h, edge_index)\n",
    "            h2 = self.ln2(h2)\n",
    "            h2 = F.relu(h2, inplace=True)\n",
    "            h  = h + 0.5 * h2  # gated residual (parameter-free)\n",
    "\n",
    "            # Global readout and classification head\n",
    "            h = global_mean_pool(h, batch)\n",
    "            return self.readout(h)\n",
    "\n",
    "    return RobustTinyGNN(input_dim, hidden_dim, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_test_graph_dataset(n_nodes: int = 2000,\n",
    "                              p_edge: float = 0.05,\n",
    "                              theta_true = (0.7, 1.2, 0.8, 0.3),\n",
    "                              seed: int = 42) -> GraphDataset:\n",
    "    \"\"\"\n",
    "    Make a toy graph with one exogenous feature per node and outcomes y from the true theta.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    G = nx.erdos_renyi_graph(n_nodes, p_edge, seed=seed)\n",
    "    A = nx.to_numpy_array(G, dtype=float)\n",
    "\n",
    "    # One scalar feature per node\n",
    "    X = rng.normal(0.0, 1.0, size=(n_nodes, 1))\n",
    "\n",
    "    # Ensure safe rho (optional; true theta already chosen benignly)\n",
    "    P = build_peer_operator(A, row_normalize=True)\n",
    "    eigvals = np.linalg.eigvals(P)\n",
    "    lambda_max = float(np.max(np.abs(eigvals))) if eigvals.size else 0.0\n",
    "    if lambda_max > 0 and abs(theta_true[3]) >= 0.99 / lambda_max:\n",
    "        rho_safe = 0.8 / lambda_max\n",
    "        theta_true = (theta_true[0], theta_true[1], theta_true[2], rho_safe)\n",
    "\n",
    "    Y = simulate_linear_in_means(X, A, np.array(theta_true), noise_std=0.1)\n",
    "    N = list(range(n_nodes))\n",
    "    return GraphDataset(X=X, Y=Y, A=A, N=N)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # True structural parameters: (alpha, beta, gamma, rho)\n",
    "    TRUE_THETA = (0.7, 1.2, 0.8, 0.3)\n",
    "\n",
    "    # 1) Data\n",
    "    data = create_test_graph_dataset(n_nodes=1000, p_edge=0.06, theta_true=TRUE_THETA, seed=123)\n",
    "\n",
    "    # 2) Bounds: loose for (alpha, beta, gamma); rho constrained by spectral radius of P\n",
    "    P = build_peer_operator(data.A, row_normalize=True)\n",
    "    eigvals = np.linalg.eigvals(P)\n",
    "    lam_max = float(np.max(np.abs(eigvals))) if eigvals.size else 1.0\n",
    "    rho_cap = (0.99 / lam_max) if lam_max > 0 else 0.99  # |rho| < 1 / lambda_max\n",
    "    bounds = [(0, 5.0), (0, 5.0), (0, 5.0), (-rho_cap, rho_cap)]\n",
    "\n",
    "    # 3) Estimator (small settings; bump up for real runs)\n",
    "    estimator = AdversarialEstimator(\n",
    "        ground_truth_data=data,\n",
    "        structural_model=structural_model,\n",
    "        initial_params=[0.0, 0.0, 0.0, 0.0],\n",
    "        bounds=bounds,\n",
    "        discriminator_factory=discriminator_factory,\n",
    "        gp_params=dict(\n",
    "            initial_point_generator=\"sobol\",\n",
    "            n_initial_points=1000,\n",
    "            n_calls=3000,\n",
    "            noise=0.10,\n",
    "            n_jobs=-1\n",
    "        ),\n",
    "    \n",
    "    )\n",
    "\n",
    "    result = estimator.estimate(m=1500, num_epochs=10, verbose=True)\n",
    "    theta_hat = result[\"x\"] if isinstance(result, dict) else result.x\n",
    "\n",
    "    # 5) Report\n",
    "    names = [\"alpha\", \"beta\", \"gamma\", \"rho\"]\n",
    "    print(\"\\n=== Estimated parameters (linear-in-means, 4 params) ===\")\n",
    "    for nm, t, th in zip(names, TRUE_THETA, theta_hat):\n",
    "        print(f\"{nm:>6s}: true = {t: .4f},  est = {th: .4f},  abs.err = {abs(th - t): .4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
